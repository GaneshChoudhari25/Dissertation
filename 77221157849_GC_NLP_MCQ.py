# -*- coding: utf-8 -*-
"""GC_NLP_MCQ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10-SSO1-pTW4PjDbcNDdW0PUZC-8oDKEq

**Mount Google Drive To Save Datasets**
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""**Install dataset & tqdm package**"""

!pip install datasets==2.0.0
!pip install tqdm==4.62.1

"""**Create Directories On Google Drive**"""

!mkdir '/content/gdrive/My Drive/t5'
!mkdir '/content/gdrive/My Drive/dataset'

!ls

"""**Load Pre-trained SQuAD Dataset From Huggingface**"""

from datasets import load_dataset
train_dataset = load_dataset('squad', split='train')
valid_dataset = load_dataset('squad', split='validation')

"""**Sample Dataset**"""

from pprint import pprint
sample_validation_dataset = next(iter(valid_dataset))
pprint (sample_validation_dataset)

context = sample_validation_dataset['context']
question = sample_validation_dataset['question']
answer = sample_validation_dataset['answers']['text'][0]

print ("context: ", context )
print ("question: ", question)
print ("answer: ", answer)

"""**Create DataFrame For Train & Validation Dataset**"""

import pandas as pd
pd.set_option("display.max_colwidth",-1)
from tqdm.notebook import tqdm
df_train = pd.DataFrame(columns = ['context', 'answer' , 'question'])
df_validation = pd.DataFrame(columns = ['context', 'answer' , 'question'])
print (df_train)
print (df_validation)

"""**Get Train Dataset Based On Answer Length**"""

count_long=0
count_short=0

for index, val in enumerate (tqdm(train_dataset)):
  text = val['context']
  question = val['question']
  answer = val['answers']['text'][0]
  no_of_words = len(answer.split())
  if no_of_words >=10:
    count_long = count_long+1
    continue
  else:
    df_train.loc[count_short]=[text] + [answer] + [question]
    count_short = count_short+1

print ("count_long train dataset: ", count_long)
print ("count_short train dataset: ", count_short)

"""**Get Validation Dataset Based On Answer Length**"""

count_long=0
count_short=0

for index, val in enumerate (tqdm(valid_dataset)):
  text = val['context']
  question = val['question']
  answer = val['answers']['text'][0]
  no_of_words = len(answer.split())
  if no_of_words >=10:
    count_long = count_long+1
    continue
  else:
    df_validation.loc[count_short]=[text] + [answer] + [question]
    count_short = count_short+1

print ("count_long validation dataset: ", count_long)
print ("count_short validation dataset: ", count_short)

from sklearn.utils import shuffle
df_train = shuffle(df_train)
df_validation = shuffle(df_validation)

"""**Check Shape Of Train & Validation Dataset**"""

print (df_train.shape)
print (df_validation.shape)

"""**Check Train Dataset**"""

df_train.head()

"""**Check Validation Dataset**"""

df_validation.head()

"""**Save Dataset To .csv Files on Google Drive**"""

train_save_path = '/content/gdrive/My Drive/dataset/squad_t5_train.csv'
valiation_save_path = '/content/gdrive/My Drive/dataset/squad_t5_valid.csv'
df_train.to_csv(train_save_path, index = False)
df_train.to_csv(valiation_save_path, index = False)

"""**Create Environment For Tokenizer & PyTorch**"""

!apt install python3.10-venv
!python -m venv .env

!source .env/bin/activate

"""**Install Tranformers, PyTorch & Tokenizer**"""

!pip install --quiet transformers==4.34.0
!pip install --quiet pytorch-lightning==1.1.3
!pip install --quiet tokenizers==0.14.1
!pip install --quiet sentencepiece==0.1.94
!pip install --quiet tqdm==4.62.1

train_file_path = '/content/gdrive/My Drive/dataset/squad_t5_train.csv'
valiation_file_path = '/content/gdrive/My Drive/dataset/squad_t5_valid.csv'

!pip install pytorch-lightning --upgrade

import argparse
import glob
import json
import time
import random
import re
from itertools import chain
from string import punctuation

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from termcolor import colored
import textwrap

from transformers import(
    AdamW,
    T5ForConditionalGeneration,
    T5Tokenizer,
    get_linear_schedule_with_warmup
)

pl.seed_everything(39)

"""**Using Pre-trained T5 Base Model**"""

t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')
t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')

"""**T5 Base Tokenizer Encoding**"""

from pprint import pprint
sample_encoding = t5_tokenizer.encode_plus("This is T5 tokenizer sample",
                                           max_length=64,
                                           pad_to_max_length=True,
                                           truncation=True,
                                           return_tensors="pt")

print (sample_encoding.keys())
pprint (sample_encoding)

print (sample_encoding['input_ids'].shape)
print (sample_encoding['input_ids'].squeeze().shape)
print (sample_encoding['input_ids'])

tokenized_output = t5_tokenizer.convert_ids_to_tokens(sample_encoding['input_ids'].squeeze())
print (tokenized_output)

"""**Check Decoding For Sample Text**"""

decoded_output = t5_tokenizer.decode(sample_encoding['input_ids'].squeeze(), skip_special_tokens=True,clean_up_tokenization_spaces=True)
print (decoded_output)

"""**T5 Base Model Vocab**"""

print (t5_tokenizer.get_vocab())
print (len(t5_tokenizer.get_vocab().keys()))

from tqdm.notebook import tqdm
import copy

class QuestionGenerationDataset(Dataset):
  def __init__(self, tokenizer, filepath, max_len_inp=512, max_len_out=100):
      self.path= filepath
      self.passage_column = "context"
      self.answer = "answer"
      self.question = "question"

      self.data = pd.read_csv(self.path,nrows=1000)
      self.max_len_input = max_len_inp
      self.max_len_output = max_len_out
      self.tokenizer = tokenizer
      self.inputs =[]
      self.targets =[]
      self.skippedcount =0
      self._build()

  def __len__(self):
      return len(self.inputs)

  def __getitem__(self, index):
      source_ids = self.inputs[index]["input_ids"].squeeze()
      target_ids = self.targets[index]["input_ids"].squeeze()

      src_mask = self.inputs[index]["attention_mask"].squeeze()
      target_mask = self.targets[index]["attention_mask"].squeeze()

      labels = copy.deepcopy(target_ids)
      labels [labels==0] = -100

      return{"source_ids": source_ids, "source_mask": src_mask, "target_ids": target_ids, "target_mask": target_mask, "labels": labels}

  def _build(self):
      for idx in tqdm(range(len(self.data))):
          passage,answer,target = self.data.loc[idx, self.passage_column], self.data.loc[idx, self.answer], self.data.loc[idx, self.question]

          input_ = "context: %s answer: %s" % (passage,answer)
          target = "question: %s </s>" % (str(target))

          test_input_encoding = self.tokenizer.encode_plus(input_,truncation=False,return_tensors="pt")

          length_of_input_encoding = len(test_input_encoding['input_ids'][0])

          if length_of_input_encoding > self.max_len_input:
            self.skippedcount = self.skippedcount+1
            continue

          tokenized_inputs = self.tokenizer.batch_encode_plus([input_], max_length=self.max_len_input, pad_to_max_length=True, return_tensors="pt")

          tokenized_targets = self.tokenizer.batch_encode_plus([target], max_length=self.max_len_output, pad_to_max_length=True, return_tensors="pt")

          self.inputs.append(tokenized_inputs)
          self.targets.append(tokenized_targets)

train_dataset = QuestionGenerationDataset(t5_tokenizer,train_file_path)

train_sample = train_dataset[50]
decoded_train_input = t5_tokenizer.convert_ids_to_tokens(train_sample['source_ids'])
decoded_train_ouput = t5_tokenizer.convert_ids_to_tokens(train_sample['target_ids'])

print (decoded_train_input)
print (decoded_train_ouput)

validation_dataset = QuestionGenerationDataset(t5_tokenizer,valiation_file_path)
